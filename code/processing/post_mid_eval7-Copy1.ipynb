{
 "metadata": {
  "name": "",
  "signature": "sha256:592904c4c87f53291ce230a7ce5f596efd1c0eb6589eba33ab2911d2ff87251f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import re\n",
      "import matplotlib.pyplot as plt\n",
      "import nltk\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn import metrics\n",
      "from sklearn import tree\n",
      "\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_pickle('./Post_Mid_Evaluation/final_feature_tweet_file')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>retweet</th>\n",
        "      <th>direct_message</th>\n",
        "      <th>username</th>\n",
        "      <th>hashtag</th>\n",
        "      <th>url</th>\n",
        "      <th>exclamation</th>\n",
        "      <th>question</th>\n",
        "      <th>positive_term</th>\n",
        "      <th>negative_term</th>\n",
        "      <th>positive_emoticon</th>\n",
        "      <th>negative_emoticon</th>\n",
        "      <th>valence</th>\n",
        "      <th>arousal</th>\n",
        "      <th>dominance</th>\n",
        "      <th>terms</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>count</th>\n",
        "      <td> 18071.000000</td>\n",
        "      <td> 18071</td>\n",
        "      <td> 18071.000000</td>\n",
        "      <td> 18071.000000</td>\n",
        "      <td> 18071.000000</td>\n",
        "      <td> 18071.000000</td>\n",
        "      <td> 18071.000000</td>\n",
        "      <td> 18071.000000</td>\n",
        "      <td> 18071.000000</td>\n",
        "      <td> 18071.000000</td>\n",
        "      <td> 18071.000000</td>\n",
        "      <td> 18071.000000</td>\n",
        "      <td> 18071.000000</td>\n",
        "      <td> 18071.000000</td>\n",
        "      <td> 18071.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mean</th>\n",
        "      <td>     0.507388</td>\n",
        "      <td>     0</td>\n",
        "      <td>     0.192131</td>\n",
        "      <td>     0.224780</td>\n",
        "      <td>     0.619446</td>\n",
        "      <td>     0.110509</td>\n",
        "      <td>     0.055559</td>\n",
        "      <td>     0.023352</td>\n",
        "      <td>     0.001605</td>\n",
        "      <td>     0.006917</td>\n",
        "      <td>     0.001937</td>\n",
        "      <td>     0.422106</td>\n",
        "      <td>    -0.474492</td>\n",
        "      <td>     0.313989</td>\n",
        "      <td>     0.528020</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>std</th>\n",
        "      <td>     0.499959</td>\n",
        "      <td>     0</td>\n",
        "      <td>     0.393986</td>\n",
        "      <td>     0.417449</td>\n",
        "      <td>     0.485537</td>\n",
        "      <td>     0.313531</td>\n",
        "      <td>     0.229074</td>\n",
        "      <td>     0.151024</td>\n",
        "      <td>     0.040029</td>\n",
        "      <td>     0.082884</td>\n",
        "      <td>     0.043968</td>\n",
        "      <td>     0.766147</td>\n",
        "      <td>     0.634337</td>\n",
        "      <td>     0.526668</td>\n",
        "      <td>     0.231426</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>min</th>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>    -3.520000</td>\n",
        "      <td>    -3.330000</td>\n",
        "      <td>    -2.860000</td>\n",
        "      <td>     0.002082</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25%</th>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>    -0.966333</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.357412</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50%</th>\n",
        "      <td>     1.000000</td>\n",
        "      <td>     0</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     1.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>    -0.090000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.507388</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>75%</th>\n",
        "      <td>     1.000000</td>\n",
        "      <td>     0</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     1.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.903167</td>\n",
        "      <td>     0.000000</td>\n",
        "      <td>     0.673333</td>\n",
        "      <td>     0.689343</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>max</th>\n",
        "      <td>     1.000000</td>\n",
        "      <td>     0</td>\n",
        "      <td>     1.000000</td>\n",
        "      <td>     1.000000</td>\n",
        "      <td>     1.000000</td>\n",
        "      <td>     1.000000</td>\n",
        "      <td>     1.000000</td>\n",
        "      <td>     1.000000</td>\n",
        "      <td>     1.000000</td>\n",
        "      <td>     1.000000</td>\n",
        "      <td>     1.000000</td>\n",
        "      <td>     3.480000</td>\n",
        "      <td>     2.450000</td>\n",
        "      <td>     2.860000</td>\n",
        "      <td>     0.999273</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "            retweet  direct_message      username       hashtag           url  \\\n",
        "count  18071.000000           18071  18071.000000  18071.000000  18071.000000   \n",
        "mean       0.507388               0      0.192131      0.224780      0.619446   \n",
        "std        0.499959               0      0.393986      0.417449      0.485537   \n",
        "min        0.000000               0      0.000000      0.000000      0.000000   \n",
        "25%        0.000000               0      0.000000      0.000000      0.000000   \n",
        "50%        1.000000               0      0.000000      0.000000      1.000000   \n",
        "75%        1.000000               0      0.000000      0.000000      1.000000   \n",
        "max        1.000000               0      1.000000      1.000000      1.000000   \n",
        "\n",
        "        exclamation      question  positive_term  negative_term  \\\n",
        "count  18071.000000  18071.000000   18071.000000   18071.000000   \n",
        "mean       0.110509      0.055559       0.023352       0.001605   \n",
        "std        0.313531      0.229074       0.151024       0.040029   \n",
        "min        0.000000      0.000000       0.000000       0.000000   \n",
        "25%        0.000000      0.000000       0.000000       0.000000   \n",
        "50%        0.000000      0.000000       0.000000       0.000000   \n",
        "75%        0.000000      0.000000       0.000000       0.000000   \n",
        "max        1.000000      1.000000       1.000000       1.000000   \n",
        "\n",
        "       positive_emoticon  negative_emoticon       valence       arousal  \\\n",
        "count       18071.000000       18071.000000  18071.000000  18071.000000   \n",
        "mean            0.006917           0.001937      0.422106     -0.474492   \n",
        "std             0.082884           0.043968      0.766147      0.634337   \n",
        "min             0.000000           0.000000     -3.520000     -3.330000   \n",
        "25%             0.000000           0.000000      0.000000     -0.966333   \n",
        "50%             0.000000           0.000000      0.000000     -0.090000   \n",
        "75%             0.000000           0.000000      0.903167      0.000000   \n",
        "max             1.000000           1.000000      3.480000      2.450000   \n",
        "\n",
        "          dominance         terms  \n",
        "count  18071.000000  18071.000000  \n",
        "mean       0.313989      0.528020  \n",
        "std        0.526668      0.231426  \n",
        "min       -2.860000      0.002082  \n",
        "25%        0.000000      0.357412  \n",
        "50%        0.000000      0.507388  \n",
        "75%        0.673333      0.689343  \n",
        "max        2.860000      0.999273  "
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#divide into training and test\n",
      "\n",
      "columns = df.columns\n",
      "columns = list(columns)\n",
      "del columns[columns.index('screenName')]\n",
      "del columns[columns.index('tweet')]\n",
      "del columns[columns.index('retweet')]\n",
      "size = int(df['url'].count())\n",
      "train = int(0.75 * size)\n",
      "test = train\n",
      "train = df[:train]\n",
      "test = df[test:]\n",
      "print train['url'].count(),test['url'].count()\n",
      "X_train = train[columns]\n",
      "y_train = df[:int(size * 0.75)]['retweet']\n",
      "X_test = test[columns]\n",
      "y_test = df[int(size * 0.75):]['retweet']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "13553 4518\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print X_train['url'].count(),y_train.count(),X_test['url'].count(),y_test.count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "13553 13553 4518 4518\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier\n",
      "train_accuracy = []\n",
      "test_accuracy = []\n",
      "roc_auc = []\n",
      "f1_score = []\n",
      "pr_auc = []\n",
      "for i in range(1,51):\n",
      "    model = None\n",
      "    model = RandomForestClassifier(n_estimators = i)\n",
      "    model.fit(X_train, y_train)\n",
      "    #print \"train accuracy\"\n",
      "    train_accuracy.append(model.score(X_train, y_train))\n",
      "    predicted_label = None\n",
      "    predicted_proba = None\n",
      "    predicted_label = model.predict(X_test)\n",
      "    predicted_proba = model.predict_proba(X_test)\n",
      "    p = [] \n",
      "    for x in predicted_proba:\n",
      "        p.append(x[1])\n",
      "\n",
      "    predicted_proba = p\n",
      "    test_accuracy.append(metrics.accuracy_score(y_test, predicted_label)*100)\n",
      "    roc_auc.append(metrics.roc_auc_score(y_test, predicted_proba))\n",
      "    f1_score.append(metrics.f1_score(y_test, predicted_label))\n",
      "    pr_auc.append(metrics.average_precision_score(y_test, predicted_proba))\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "random_forest_results = zip(range(1,51),train_accuracy,test_accuracy,roc_auc,f1_score,pr_auc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_results = pd.DataFrame(random_forest_results, columns = ['No. of Trees','train accuracy','test accuracy','ROC AUC','F1-Score','PR AUC'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>No. of Trees</th>\n",
        "      <th>train accuracy</th>\n",
        "      <th>test accuracy</th>\n",
        "      <th>ROC AUC</th>\n",
        "      <th>F1-Score</th>\n",
        "      <th>PR AUC</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0 </th>\n",
        "      <td>  1</td>\n",
        "      <td> 0.859293</td>\n",
        "      <td> 78.198318</td>\n",
        "      <td> 0.827229</td>\n",
        "      <td> 0.780672</td>\n",
        "      <td> 0.843104</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1 </th>\n",
        "      <td>  2</td>\n",
        "      <td> 0.862540</td>\n",
        "      <td> 79.703409</td>\n",
        "      <td> 0.873836</td>\n",
        "      <td> 0.787190</td>\n",
        "      <td> 0.873716</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2 </th>\n",
        "      <td>  3</td>\n",
        "      <td> 0.882904</td>\n",
        "      <td> 82.536521</td>\n",
        "      <td> 0.888318</td>\n",
        "      <td> 0.798673</td>\n",
        "      <td> 0.887567</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3 </th>\n",
        "      <td>  4</td>\n",
        "      <td> 0.885339</td>\n",
        "      <td> 81.164232</td>\n",
        "      <td> 0.895047</td>\n",
        "      <td> 0.803872</td>\n",
        "      <td> 0.894544</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4 </th>\n",
        "      <td>  5</td>\n",
        "      <td> 0.893750</td>\n",
        "      <td> 81.341301</td>\n",
        "      <td> 0.896844</td>\n",
        "      <td> 0.811956</td>\n",
        "      <td> 0.893758</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5 </th>\n",
        "      <td>  6</td>\n",
        "      <td> 0.892422</td>\n",
        "      <td> 81.341301</td>\n",
        "      <td> 0.899728</td>\n",
        "      <td> 0.809578</td>\n",
        "      <td> 0.895073</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6 </th>\n",
        "      <td>  7</td>\n",
        "      <td> 0.895964</td>\n",
        "      <td> 81.983178</td>\n",
        "      <td> 0.905409</td>\n",
        "      <td> 0.818060</td>\n",
        "      <td> 0.900224</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7 </th>\n",
        "      <td>  8</td>\n",
        "      <td> 0.896702</td>\n",
        "      <td> 81.518371</td>\n",
        "      <td> 0.900821</td>\n",
        "      <td> 0.811129</td>\n",
        "      <td> 0.894928</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8 </th>\n",
        "      <td>  9</td>\n",
        "      <td> 0.896628</td>\n",
        "      <td> 82.115981</td>\n",
        "      <td> 0.905633</td>\n",
        "      <td> 0.820285</td>\n",
        "      <td> 0.900256</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9 </th>\n",
        "      <td> 10</td>\n",
        "      <td> 0.898251</td>\n",
        "      <td> 81.983178</td>\n",
        "      <td> 0.907147</td>\n",
        "      <td> 0.816501</td>\n",
        "      <td> 0.899863</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10</th>\n",
        "      <td> 11</td>\n",
        "      <td> 0.899653</td>\n",
        "      <td> 82.182382</td>\n",
        "      <td> 0.908642</td>\n",
        "      <td> 0.820433</td>\n",
        "      <td> 0.899668</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11</th>\n",
        "      <td> 12</td>\n",
        "      <td> 0.899801</td>\n",
        "      <td> 81.872510</td>\n",
        "      <td> 0.906472</td>\n",
        "      <td> 0.816573</td>\n",
        "      <td> 0.898916</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12</th>\n",
        "      <td> 13</td>\n",
        "      <td> 0.900908</td>\n",
        "      <td> 82.093847</td>\n",
        "      <td> 0.908772</td>\n",
        "      <td> 0.819942</td>\n",
        "      <td> 0.900996</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13</th>\n",
        "      <td> 14</td>\n",
        "      <td> 0.901498</td>\n",
        "      <td> 81.806109</td>\n",
        "      <td> 0.908771</td>\n",
        "      <td> 0.816272</td>\n",
        "      <td> 0.902021</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>14</th>\n",
        "      <td> 15</td>\n",
        "      <td> 0.902014</td>\n",
        "      <td> 82.514387</td>\n",
        "      <td> 0.911243</td>\n",
        "      <td> 0.823975</td>\n",
        "      <td> 0.902298</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>15</th>\n",
        "      <td> 16</td>\n",
        "      <td> 0.901055</td>\n",
        "      <td> 82.005312</td>\n",
        "      <td> 0.909685</td>\n",
        "      <td> 0.817344</td>\n",
        "      <td> 0.901890</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>16</th>\n",
        "      <td> 17</td>\n",
        "      <td> 0.903564</td>\n",
        "      <td> 82.447986</td>\n",
        "      <td> 0.912731</td>\n",
        "      <td> 0.823582</td>\n",
        "      <td> 0.901128</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>17</th>\n",
        "      <td> 18</td>\n",
        "      <td> 0.902014</td>\n",
        "      <td> 82.580788</td>\n",
        "      <td> 0.912074</td>\n",
        "      <td> 0.824055</td>\n",
        "      <td> 0.904618</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>18</th>\n",
        "      <td> 19</td>\n",
        "      <td> 0.902900</td>\n",
        "      <td> 82.514387</td>\n",
        "      <td> 0.910553</td>\n",
        "      <td> 0.823345</td>\n",
        "      <td> 0.902928</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>19</th>\n",
        "      <td> 20</td>\n",
        "      <td> 0.903416</td>\n",
        "      <td> 82.182382</td>\n",
        "      <td> 0.911009</td>\n",
        "      <td> 0.820112</td>\n",
        "      <td> 0.902136</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>20</th>\n",
        "      <td> 21</td>\n",
        "      <td> 0.902605</td>\n",
        "      <td> 82.248783</td>\n",
        "      <td> 0.911257</td>\n",
        "      <td> 0.820662</td>\n",
        "      <td> 0.900086</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>21</th>\n",
        "      <td> 22</td>\n",
        "      <td> 0.904080</td>\n",
        "      <td> 82.381585</td>\n",
        "      <td> 0.911833</td>\n",
        "      <td> 0.822321</td>\n",
        "      <td> 0.902237</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>22</th>\n",
        "      <td> 23</td>\n",
        "      <td> 0.904006</td>\n",
        "      <td> 82.337317</td>\n",
        "      <td> 0.910796</td>\n",
        "      <td> 0.822430</td>\n",
        "      <td> 0.901570</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>23</th>\n",
        "      <td> 24</td>\n",
        "      <td> 0.903933</td>\n",
        "      <td> 82.403718</td>\n",
        "      <td> 0.911444</td>\n",
        "      <td> 0.822663</td>\n",
        "      <td> 0.899750</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>24</th>\n",
        "      <td> 25</td>\n",
        "      <td> 0.904449</td>\n",
        "      <td> 82.536521</td>\n",
        "      <td> 0.913592</td>\n",
        "      <td> 0.824706</td>\n",
        "      <td> 0.903501</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25</th>\n",
        "      <td> 26</td>\n",
        "      <td> 0.903564</td>\n",
        "      <td> 82.381585</td>\n",
        "      <td> 0.911796</td>\n",
        "      <td> 0.822321</td>\n",
        "      <td> 0.901868</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>26</th>\n",
        "      <td> 27</td>\n",
        "      <td> 0.904080</td>\n",
        "      <td> 82.226649</td>\n",
        "      <td> 0.911622</td>\n",
        "      <td> 0.821278</td>\n",
        "      <td> 0.899776</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>27</th>\n",
        "      <td> 28</td>\n",
        "      <td> 0.904818</td>\n",
        "      <td> 82.403718</td>\n",
        "      <td> 0.913253</td>\n",
        "      <td> 0.822584</td>\n",
        "      <td> 0.904265</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>28</th>\n",
        "      <td> 29</td>\n",
        "      <td> 0.904744</td>\n",
        "      <td> 82.514387</td>\n",
        "      <td> 0.914604</td>\n",
        "      <td> 0.823818</td>\n",
        "      <td> 0.907272</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>29</th>\n",
        "      <td> 30</td>\n",
        "      <td> 0.904449</td>\n",
        "      <td> 82.359451</td>\n",
        "      <td> 0.912882</td>\n",
        "      <td> 0.821501</td>\n",
        "      <td> 0.902986</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>30</th>\n",
        "      <td> 31</td>\n",
        "      <td> 0.904892</td>\n",
        "      <td> 82.337317</td>\n",
        "      <td> 0.912388</td>\n",
        "      <td> 0.822351</td>\n",
        "      <td> 0.902130</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>31</th>\n",
        "      <td> 32</td>\n",
        "      <td> 0.904597</td>\n",
        "      <td> 82.160248</td>\n",
        "      <td> 0.913942</td>\n",
        "      <td> 0.820089</td>\n",
        "      <td> 0.903595</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>32</th>\n",
        "      <td> 33</td>\n",
        "      <td> 0.904744</td>\n",
        "      <td> 82.160248</td>\n",
        "      <td> 0.911985</td>\n",
        "      <td> 0.820570</td>\n",
        "      <td> 0.900580</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>33</th>\n",
        "      <td> 34</td>\n",
        "      <td> 0.904744</td>\n",
        "      <td> 82.403718</td>\n",
        "      <td> 0.912100</td>\n",
        "      <td> 0.822267</td>\n",
        "      <td> 0.901053</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>34</th>\n",
        "      <td> 35</td>\n",
        "      <td> 0.905187</td>\n",
        "      <td> 82.425852</td>\n",
        "      <td> 0.914947</td>\n",
        "      <td> 0.823241</td>\n",
        "      <td> 0.906231</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>35</th>\n",
        "      <td> 36</td>\n",
        "      <td> 0.905039</td>\n",
        "      <td> 82.071713</td>\n",
        "      <td> 0.911088</td>\n",
        "      <td> 0.819196</td>\n",
        "      <td> 0.901309</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>36</th>\n",
        "      <td> 37</td>\n",
        "      <td> 0.905482</td>\n",
        "      <td> 82.602922</td>\n",
        "      <td> 0.915288</td>\n",
        "      <td> 0.824788</td>\n",
        "      <td> 0.903353</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>37</th>\n",
        "      <td> 38</td>\n",
        "      <td> 0.905187</td>\n",
        "      <td> 82.315184</td>\n",
        "      <td> 0.912496</td>\n",
        "      <td> 0.821692</td>\n",
        "      <td> 0.900599</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>38</th>\n",
        "      <td> 39</td>\n",
        "      <td> 0.905113</td>\n",
        "      <td> 82.425852</td>\n",
        "      <td> 0.912467</td>\n",
        "      <td> 0.823005</td>\n",
        "      <td> 0.901813</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>39</th>\n",
        "      <td> 40</td>\n",
        "      <td> 0.905113</td>\n",
        "      <td> 82.270916</td>\n",
        "      <td> 0.913676</td>\n",
        "      <td> 0.821405</td>\n",
        "      <td> 0.906965</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>40</th>\n",
        "      <td> 41</td>\n",
        "      <td> 0.905335</td>\n",
        "      <td> 82.403718</td>\n",
        "      <td> 0.913496</td>\n",
        "      <td> 0.822742</td>\n",
        "      <td> 0.902321</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>41</th>\n",
        "      <td> 42</td>\n",
        "      <td> 0.904818</td>\n",
        "      <td> 82.160248</td>\n",
        "      <td> 0.910638</td>\n",
        "      <td> 0.819929</td>\n",
        "      <td> 0.898726</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>42</th>\n",
        "      <td> 43</td>\n",
        "      <td> 0.905187</td>\n",
        "      <td> 82.359451</td>\n",
        "      <td> 0.913271</td>\n",
        "      <td> 0.822138</td>\n",
        "      <td> 0.901851</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>43</th>\n",
        "      <td> 44</td>\n",
        "      <td> 0.905704</td>\n",
        "      <td> 82.270916</td>\n",
        "      <td> 0.913410</td>\n",
        "      <td> 0.821564</td>\n",
        "      <td> 0.902017</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>44</th>\n",
        "      <td> 45</td>\n",
        "      <td> 0.905482</td>\n",
        "      <td> 82.580788</td>\n",
        "      <td> 0.914627</td>\n",
        "      <td> 0.824683</td>\n",
        "      <td> 0.901288</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>45</th>\n",
        "      <td> 46</td>\n",
        "      <td> 0.905556</td>\n",
        "      <td> 82.403718</td>\n",
        "      <td> 0.913074</td>\n",
        "      <td> 0.822821</td>\n",
        "      <td> 0.902189</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>46</th>\n",
        "      <td> 47</td>\n",
        "      <td> 0.905408</td>\n",
        "      <td> 82.403718</td>\n",
        "      <td> 0.912695</td>\n",
        "      <td> 0.822979</td>\n",
        "      <td> 0.901501</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>47</th>\n",
        "      <td> 48</td>\n",
        "      <td> 0.904892</td>\n",
        "      <td> 82.403718</td>\n",
        "      <td> 0.912692</td>\n",
        "      <td> 0.822267</td>\n",
        "      <td> 0.900456</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>48</th>\n",
        "      <td> 49</td>\n",
        "      <td> 0.904744</td>\n",
        "      <td> 82.293050</td>\n",
        "      <td> 0.913339</td>\n",
        "      <td> 0.821826</td>\n",
        "      <td> 0.901333</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>49</th>\n",
        "      <td> 50</td>\n",
        "      <td> 0.905630</td>\n",
        "      <td> 82.359451</td>\n",
        "      <td> 0.912547</td>\n",
        "      <td> 0.822850</td>\n",
        "      <td> 0.901323</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 41,
       "text": [
        "    No. of Trees  train accuracy  test accuracy   ROC AUC  F1-Score    PR AUC\n",
        "0              1        0.859293      78.198318  0.827229  0.780672  0.843104\n",
        "1              2        0.862540      79.703409  0.873836  0.787190  0.873716\n",
        "2              3        0.882904      82.536521  0.888318  0.798673  0.887567\n",
        "3              4        0.885339      81.164232  0.895047  0.803872  0.894544\n",
        "4              5        0.893750      81.341301  0.896844  0.811956  0.893758\n",
        "5              6        0.892422      81.341301  0.899728  0.809578  0.895073\n",
        "6              7        0.895964      81.983178  0.905409  0.818060  0.900224\n",
        "7              8        0.896702      81.518371  0.900821  0.811129  0.894928\n",
        "8              9        0.896628      82.115981  0.905633  0.820285  0.900256\n",
        "9             10        0.898251      81.983178  0.907147  0.816501  0.899863\n",
        "10            11        0.899653      82.182382  0.908642  0.820433  0.899668\n",
        "11            12        0.899801      81.872510  0.906472  0.816573  0.898916\n",
        "12            13        0.900908      82.093847  0.908772  0.819942  0.900996\n",
        "13            14        0.901498      81.806109  0.908771  0.816272  0.902021\n",
        "14            15        0.902014      82.514387  0.911243  0.823975  0.902298\n",
        "15            16        0.901055      82.005312  0.909685  0.817344  0.901890\n",
        "16            17        0.903564      82.447986  0.912731  0.823582  0.901128\n",
        "17            18        0.902014      82.580788  0.912074  0.824055  0.904618\n",
        "18            19        0.902900      82.514387  0.910553  0.823345  0.902928\n",
        "19            20        0.903416      82.182382  0.911009  0.820112  0.902136\n",
        "20            21        0.902605      82.248783  0.911257  0.820662  0.900086\n",
        "21            22        0.904080      82.381585  0.911833  0.822321  0.902237\n",
        "22            23        0.904006      82.337317  0.910796  0.822430  0.901570\n",
        "23            24        0.903933      82.403718  0.911444  0.822663  0.899750\n",
        "24            25        0.904449      82.536521  0.913592  0.824706  0.903501\n",
        "25            26        0.903564      82.381585  0.911796  0.822321  0.901868\n",
        "26            27        0.904080      82.226649  0.911622  0.821278  0.899776\n",
        "27            28        0.904818      82.403718  0.913253  0.822584  0.904265\n",
        "28            29        0.904744      82.514387  0.914604  0.823818  0.907272\n",
        "29            30        0.904449      82.359451  0.912882  0.821501  0.902986\n",
        "30            31        0.904892      82.337317  0.912388  0.822351  0.902130\n",
        "31            32        0.904597      82.160248  0.913942  0.820089  0.903595\n",
        "32            33        0.904744      82.160248  0.911985  0.820570  0.900580\n",
        "33            34        0.904744      82.403718  0.912100  0.822267  0.901053\n",
        "34            35        0.905187      82.425852  0.914947  0.823241  0.906231\n",
        "35            36        0.905039      82.071713  0.911088  0.819196  0.901309\n",
        "36            37        0.905482      82.602922  0.915288  0.824788  0.903353\n",
        "37            38        0.905187      82.315184  0.912496  0.821692  0.900599\n",
        "38            39        0.905113      82.425852  0.912467  0.823005  0.901813\n",
        "39            40        0.905113      82.270916  0.913676  0.821405  0.906965\n",
        "40            41        0.905335      82.403718  0.913496  0.822742  0.902321\n",
        "41            42        0.904818      82.160248  0.910638  0.819929  0.898726\n",
        "42            43        0.905187      82.359451  0.913271  0.822138  0.901851\n",
        "43            44        0.905704      82.270916  0.913410  0.821564  0.902017\n",
        "44            45        0.905482      82.580788  0.914627  0.824683  0.901288\n",
        "45            46        0.905556      82.403718  0.913074  0.822821  0.902189\n",
        "46            47        0.905408      82.403718  0.912695  0.822979  0.901501\n",
        "47            48        0.904892      82.403718  0.912692  0.822267  0.900456\n",
        "48            49        0.904744      82.293050  0.913339  0.821826  0.901333\n",
        "49            50        0.905630      82.359451  0.912547  0.822850  0.901323"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_results[df_results['ROC AUC'] == df_results['ROC AUC'].max()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>No. of Trees</th>\n",
        "      <th>train accuracy</th>\n",
        "      <th>test accuracy</th>\n",
        "      <th>ROC AUC</th>\n",
        "      <th>F1-Score</th>\n",
        "      <th>PR AUC</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>36</th>\n",
        "      <td> 37</td>\n",
        "      <td> 0.905482</td>\n",
        "      <td> 82.602922</td>\n",
        "      <td> 0.915288</td>\n",
        "      <td> 0.824788</td>\n",
        "      <td> 0.903353</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 42,
       "text": [
        "    No. of Trees  train accuracy  test accuracy   ROC AUC  F1-Score    PR AUC\n",
        "36            37        0.905482      82.602922  0.915288  0.824788  0.903353"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_results[df_results['test accuracy'] == df_results['test accuracy'].max()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>No. of Trees</th>\n",
        "      <th>train accuracy</th>\n",
        "      <th>test accuracy</th>\n",
        "      <th>ROC AUC</th>\n",
        "      <th>F1-Score</th>\n",
        "      <th>PR AUC</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>36</th>\n",
        "      <td> 37</td>\n",
        "      <td> 0.905482</td>\n",
        "      <td> 82.602922</td>\n",
        "      <td> 0.915288</td>\n",
        "      <td> 0.824788</td>\n",
        "      <td> 0.903353</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 43,
       "text": [
        "    No. of Trees  train accuracy  test accuracy   ROC AUC  F1-Score    PR AUC\n",
        "36            37        0.905482      82.602922  0.915288  0.824788  0.903353"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_results.to_csv('./Post_Mid_Evaluation/random_forest/comparison_of_results.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_results.to_pickle('./Post_Mid_Evaluation/random_forest/comparison_of_results')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = None\n",
      "model = RandomForestClassifier(n_estimators = 37)\n",
      "model.fit(X_train, y_train)\n",
      "print \"train accuracy\"\n",
      "print model.score(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "train accuracy\n",
        "0.904818121449"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predicted_label = None\n",
      "predicted_proba = None\n",
      "predicted_label = model.predict(X_test)\n",
      "predicted_proba = model.predict_proba(X_test)\n",
      "p = [] \n",
      "for x in predicted_proba:\n",
      "   p.append(x[1])\n",
      "\n",
      "predicted_proba = p"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"accuracy\"\n",
      "print (metrics.accuracy_score(y_test, predicted_label)*100),\"%\"\n",
      "print \"better than random classifier with accuracy \"\n",
      "print y_test.mean()*100,\"%\"\n",
      "print \"\\nROC AUC\"\n",
      "print metrics.roc_auc_score(y_test, predicted_proba)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "accuracy\n",
        "82.7799911465 %\n",
        "better than random classifier with accuracy \n",
        "46.768481629 %\n",
        "\n",
        "ROC AUC\n",
        "0.914364202201\n"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print metrics.confusion_matrix(y_test, predicted_label)\n",
      "print metrics.classification_report(y_test, predicted_label)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[1884  521]\n",
        " [ 296 1817]]\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.86      0.78      0.82      2405\n",
        "          1       0.78      0.86      0.82      2113\n",
        "\n",
        "avg / total       0.82      0.82      0.82      4518\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_roc(algo_name, targets, scores, ph):\n",
      "    fpr, tpr, thresholds = metrics.roc_curve(targets, scores)\n",
      "    roc_auc = metrics.auc(fpr, tpr)\n",
      "    fig = ph.figure()\n",
      "    ax = fig.add_subplot(111)\n",
      "    fig.subplots_adjust(top=1)\n",
      "    ax.set_title('ROC Curve with '+algo_name)\n",
      "    ax.text(0.95, 0.01, 'Area Under Curve : (%0.4f)' % roc_auc,\n",
      "        verticalalignment='bottom', horizontalalignment='right',\n",
      "        transform=ax.transAxes,\n",
      "        color='green', fontsize=15)\n",
      "    ph.plot(fpr, tpr, label=str(algo_name) + '(%0.4f)' % roc_auc)\n",
      "    ph.show()\n",
      "    #return roc_auc\n",
      "\n",
      "   \n",
      "plot_roc('Random Forest', y_test, predicted_proba, plt)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEnCAYAAABSTgMJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ni4jEIYW7iEgc+v+Cntjr7uUuvwAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x183fac88>"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svc_output = zip(y_test,predicted_label,predicted_proba)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svc_output = pd.DataFrame(svc_output, columns = ['actual','predicted','probability'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svc_output[1:15]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>actual</th>\n",
        "      <th>predicted</th>\n",
        "      <th>probability</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>1 </th>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0.504922</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2 </th>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0.504922</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3 </th>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0.700000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4 </th>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0.504922</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5 </th>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0.504922</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6 </th>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0.954947</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7 </th>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td> 1.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8 </th>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0.700000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9 </th>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0.100000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10</th>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0.504922</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11</th>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0.900000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12</th>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0.504922</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13</th>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> 1.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>14</th>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0.000000</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "    actual  predicted  probability\n",
        "1        1          1     0.504922\n",
        "2        1          1     0.504922\n",
        "3        0          1     0.700000\n",
        "4        1          1     0.504922\n",
        "5        0          1     0.504922\n",
        "6        1          1     0.954947\n",
        "7        0          1     1.000000\n",
        "8        0          1     0.700000\n",
        "9        0          0     0.100000\n",
        "10       1          1     0.504922\n",
        "11       1          1     0.900000\n",
        "12       0          1     0.504922\n",
        "13       1          1     1.000000\n",
        "14       0          0     0.000000"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svc_output.to_pickle('./Post_Mid_Evaluation/random_forest/random_forest_output')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}